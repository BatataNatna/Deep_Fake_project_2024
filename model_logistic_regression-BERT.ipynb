{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "TOKENIZER_ID = config['TOKENIZER_ID']\n",
    "nrows = None if config['nrows'] == 0 else config['nrows']\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split # to separate the dataset.\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import make_scorer,classification_report, accuracy_score,confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42834, 3)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('title_text.csv',nrows=nrows).drop(['Unnamed: 0'],axis=1)\n",
    "data.head()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification du text en données numériques. (BERT)\n",
    "Nous partons du principe que les données ont étés clean et que nous pouvons nous concerntrer à créer un model.\n",
    "\n",
    "Comme les informations principales pour définir des Fake news et des vrai news, se base sur le texte et le titre, nous devons nous focaliser sur ces informations. Il faut ainsi que nous transformons le text et le titre en données afin que notre model puisse utiliser des données numériques pour trouver la bonne réponse. Nous allons utiliser le BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [101, 2005, 3071, 2060, 2084, 2672, 2010, 5211...\n",
      "1        [101, 8755, 1006, 26665, 1007, 1011, 1037, 764...\n",
      "2        [101, 4199, 1006, 26665, 1007, 1011, 2280, 353...\n",
      "3        [101, 1037, 9410, 2277, 2326, 2012, 1996, 2358...\n",
      "4        [101, 1996, 8398, 2317, 2160, 2003, 2085, 3985...\n",
      "                               ...                        \n",
      "42829    [101, 1006, 26665, 1007, 1011, 6041, 7206, 270...\n",
      "42830    [101, 2085, 2008, 6221, 8398, 2038, 2915, 2370...\n",
      "42831    [101, 1996, 2214, 10539, 2962, 2087, 4141, 210...\n",
      "42832    [101, 2899, 1006, 26665, 1007, 1011, 4883, 401...\n",
      "42833    [101, 2899, 1006, 26665, 1007, 1011, 1057, 101...\n",
      "Name: text, Length: 42834, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = data['text']\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the maximum sequence length\n",
    "max_seq_length = 512\n",
    "\n",
    "# Truncate or pad the tokenized sequences\n",
    "X = X.apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_seq_length, truncation=True, padding='max_length'))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    21417\n",
      "0    21417\n",
      "Name: isFake, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = data['isFake'].astype(int) #make it numerical\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we separate the trdata into training test and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34267, 512) (34267,)\n",
      "(4283, 512) (4283,)\n",
      "(4284, 512) (4284,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train.tolist())\n",
    "y_train= np.array(y_train.tolist())\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "X_test = np.array(X_test.tolist())\n",
    "y_test= np.array(y_test.tolist())\n",
    "print(X_test.shape,y_test.shape)\n",
    "\n",
    "X_validation = np.array(X_validation.tolist())\n",
    "y_validation= np.array(y_validation.tolist())\n",
    "print(X_validation.shape,y_validation.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test Data: 82.93252393182348%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MIKED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "Accuracy = logreg.score(X_test, y_test)\n",
    "print(\"Accuracy Test Data: \"+str(Accuracy*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Validation Data: 83.73015873015873%\n"
     ]
    }
   ],
   "source": [
    "y_pred_validation=logreg.predict(X_validation)\n",
    "\n",
    "# we calculate the accuracy of the model on the validation data\n",
    "accuracy_validation=accuracy_score(y_validation, y_pred_validation)\n",
    "print(\"Accuracy Validation Data: \"+str(accuracy_validation*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization\n",
    "We create a grid search algorithm to find the best hyperparameters for the model.\n",
    "The model is a logistic regression model with the following hyperparameters:\n",
    "- C: the regularization strength, in simple terms, it is used to avoid overfitting. A low value of C means that the model will try to adjust to the majority of the data points, while a high value of C means that the model will try to adjust to the minority of the data points.\n",
    "- max_iter: the maximum number of iterations, it is used to avoid overfitting. A low value of max_iter means that the model will try to adjust to the majority of the data points, while a high value of max_iter means that the model will try to adjust to the minority of the data points.\n",
    "\n",
    "these parameters will be tested on the following values (randomly chosen):\n",
    "- C: [0.01,0.05,0.1,0.5,1,5, 10,50,100,1000]\n",
    "- max_iter: [100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   50.6s\n"
     ]
    }
   ],
   "source": [
    "params_grid = {'C': [0.01,0.05,0.1,0.5,1,5], \n",
    "               'max_iter': [10,50,100,500,1000,1500]\n",
    "               }\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "# On creer un scorer pour le grid search \n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Initialisation de la GridSearch pour trouver le meilleur C.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model, \n",
    "    param_grid=params_grid, # parametres à tester\n",
    "    scoring=scorer, # scorer\n",
    "    cv=5, # cv=5 pour utiliser la validation croisée à 5 folds \n",
    "    verbose=2, # verbose=2 pour afficher les logs\n",
    "    n_jobs=-1, # n_jobs=-1 pour utiliser tous les coeurs du CPU\n",
    "    return_train_score=True # pour afficher les scores d'entrainement\n",
    "    )\n",
    "\n",
    "# On lance les multiples entrainements.\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best combination of parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On recupere le meilleur modele\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On predit les donnees de test\n",
    "y_pred_test = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule l'accuracy\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy Test Data: {accuracy_test*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On predit les donnees de validation\n",
    "y_pred_validation = best_model.predict(X_validation)\n",
    "# On calcule l'accuracy\n",
    "accuracy_validation = accuracy_score(y_validation, y_pred_validation)\n",
    "print(f\"Accuracy Validation Data: {accuracy_validation*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on test toute les autres metric possible pour évaluer notre model de naive bayes.\n",
    "\n",
    "# on fait le classification_report\n",
    "print(classification_report(y_validation, y_pred_validation))\n",
    "#on obtient la precision,recall et f1_score\n",
    "precision = precision_score(y_validation, y_pred_validation)\n",
    "recall = recall_score(y_validation, y_pred_validation)\n",
    "f1 = f1_score(y_validation, y_pred_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche la matrice de confusion en utilisant matplotlib\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_test), annot=True, fmt='d',)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche la courbe ROC\n",
    "\n",
    "#on obtient y_pred \n",
    "y_pred = best_model.predict_proba(X_test)[:, 1]\n",
    "#on obtient la valeur roc_auc\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('Taux de Faux Positives (%)')\n",
    "plt.ylabel('Taux de Vrai Positives (%)')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de performances sur des données générées par Téléchargé ailleurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data=pd.read_csv(\"data/validation_data.csv\",nrows=nrows)\n",
    "validation_data=validation_data[['text','isFake']]\n",
    "\n",
    "# Prepare the data\n",
    "validation_data_X = validation_data['text'].apply(lambda x: x.lower())  # the text data\n",
    "validation_data_y = validation_data['isFake']\n",
    "\n",
    "# Tokenize the data using the BERT tokenizer\n",
    "encoded_inputs = tokenizer(validation_data_X.tolist(), padding=True, truncation=True, max_length=max_seq_length, return_tensors='np')\n",
    "\n",
    "# Get the input IDs and attention masks\n",
    "input_ids = encoded_inputs['input_ids']\n",
    "attention_masks = encoded_inputs['attention_mask']\n",
    "\n",
    "# Use the trained model to make predictions on the dataset\n",
    "predictions = best_model.predict(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f153d52fbd1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#we print the accuracy of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Accuracy mew Validation Data: {accuracy*100}%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "#we print the accuracy of the model\n",
    "accuracy = accuracy_score(validation_data_y, predictions)\n",
    "print(f\"Accuracy different Dataset: {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(validation_data_y,predictions)\n",
    "#représentation graphique du résultat du meilleure model\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Les labels prédits')\n",
    "plt.ylabel('Les vrais labels')\n",
    "plt.title('Matrice de confusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we print the percentage of True Positive, True Negative, False Positive and False Negative\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"True Negative: {tn}\")\n",
    "print(f\"False Positive: {fp}\")\n",
    "print(f\"False Negative: {fn}\")\n",
    "print(f\"True Positive: {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
