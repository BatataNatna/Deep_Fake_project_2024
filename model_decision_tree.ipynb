{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score, confusion_matrix, roc_auc_score, roc_curve # this is used to evaluate the model.\n",
    "from sklearn.model_selection import train_test_split # to separate the dataset.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #used for the transformation of text data.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time #pour étudier le temps d'execution d'entrainement, de notre meilleure modèle.\n",
    "import psutil #pour calculer la mémoire.\n",
    "import os #pour calculer le temps d'execution\n",
    "import pickle as pkl #file to save and load a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_processed</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>isFake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white hous threaten veto senat defens bill</td>\n",
       "      <td>washington reuter white hous threaten tuesday ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>colleg socialist group incit child yell kill d...</td>\n",
       "      <td>knight social hold event allow student bash pi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kuwait could pay parti trump hotel washington</td>\n",
       "      <td>correct feb stori show event take place wednes...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>professor claim sexual harass ban violat first...</td>\n",
       "      <td>group colleg professor releas report claim fed...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump say role we libya defeat islam state</td>\n",
       "      <td>washington reuter presid donald trump say thur...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title_processed  \\\n",
       "0         white hous threaten veto senat defens bill   \n",
       "1  colleg socialist group incit child yell kill d...   \n",
       "2      kuwait could pay parti trump hotel washington   \n",
       "3  professor claim sexual harass ban violat first...   \n",
       "4         trump say role we libya defeat islam state   \n",
       "\n",
       "                                      text_processed  isFake  \n",
       "0  washington reuter white hous threaten tuesday ...   False  \n",
       "1  knight social hold event allow student bash pi...    True  \n",
       "2  correct feb stori show event take place wednes...   False  \n",
       "3  group colleg professor releas report claim fed...    True  \n",
       "4  washington reuter presid donald trump say thur...   False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('title_text.csv').drop(['Unnamed: 0'],axis=1).iloc[:,:150]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification du text en données numériques.\n",
    "Nous partons du principe que les données ont étés clean et que nous pouvons nous concerntrer à créer un model.\n",
    "\n",
    "Comme les informations principales pour définir des Fake news et des vrai news, se base sur le texte et le titre, nous devons nous focaliser sur ces informations. Il faut ainsi que nous transformons le text et le titre en données afin que notre model puisse utiliser des données numériques pour trouver la bonne réponse. Nous allons utiliser la methode de TF-IDF vectorizing.\n",
    "\n",
    "### TF-IDF vectorizer\n",
    "Cette méthode offre la possibilité de déterminer les scores TF-IDF de nos données textuelles. Ce qui signifie : \n",
    "\n",
    "TF -> la Fréquence du Terme : le nombre de fois qu'un mot apparaît, en éliminant les mots qui ne sont pas importants.\n",
    "\n",
    "IDF -> la fréquence inverse de détection : le même concept que TF, mais l'analyse va au-delà de la simple ligne, la fréquence dans l'ensemble du dataset est analysée.\n",
    "\n",
    "Ces informations seront utilisées comme données pour notre modèle d'apprentissage automatique.\n",
    "\n",
    "(il est important de faire cela uniquement sur nos données d'entraînement, sinon le modèle connaîtra les données de test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title_processed', 'text_processed', 'isFake'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 27.5 GiB for an array with shape (29116, 126692) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m tfidf_vectorizer\u001b[38;5;241m=\u001b[39mTfidfVectorizer(use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# On fit et transforme les données d'entrainement.\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m tfidf_train\u001b[38;5;241m=\u001b[39mtfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# on transfomer uniquement le test set, sinon il va connaitre les données test...\u001b[39;00m\n\u001b[0;32m     12\u001b[0m tfidf_test\u001b[38;5;241m=\u001b[39mtfidf_vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 27.5 GiB for an array with shape (29116, 126692) and data type float64"
     ]
    }
   ],
   "source": [
    "X=data.drop(['isFake'],axis=1)\n",
    "y=data['isFake']\n",
    "#spearation en données d'entrainement et de test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['text_processed'], y, test_size=0.3, random_state=42)\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "\n",
    "# On fit et transforme les données d'entrainement.\n",
    "tfidf_train=tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "\n",
    "# on transfomer uniquement le test set, sinon il va connaitre les données test...\n",
    "tfidf_test=tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model decision tree\n",
    "Un arbre de décision est une technique d'apprentissage supervisé utilisée pour résoudre des problèmes de classification et de régression. Il vise à modéliser la relation entre une variable cible, qui dans notre cas sont les Fake news, et plusieurs variables explicatives, dans ce cas nous utiliserons le texte de la news en question.\n",
    "\n",
    "L'approche de l'arbre de décision suit la stratégie de **\"diviser pour conquérir\"**.\n",
    "\n",
    "Cet algorithme itératif construit progressivement l'arbre. Au départ, il sélectionne l'attribut qui divise le mieux les données, formant des groupes en relation avec la variable cible. À chaque étape suivante, il répète ce processus pour chaque sous-ensemble créé précédemment, en se basant sur les nœuds déjà formés, jusqu'à ce que les critères de terminaison soient satisfaits (ex: le nombre maximal de niveaux dans l'arbre)\n",
    "\n",
    "Une fois l'arbre construit, il peut être utilisé pour prédire l'issue pour de nouvelles données en les faisant passer à travers l'arbre, de la racine jusqu'à une feuille, qui représente la décision finale ou la prédiction de la variable cible.\n",
    "\n",
    "src: https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "#nous construisons le modèle\n",
    "clf = DecisionTreeClassifier(max_depth=1,random_state=42)\n",
    "#nous l'entrainons\n",
    "clf.fit(tfidf_train, y_train)\n",
    "\n",
    "#nous obtenons une accuracy du modèle\n",
    "Accuracy = clf.score(tfidf_test, y_test)\n",
    "print(Accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation\n",
    "Pour optimiser le model nous allons modifier les paramètres suivants:\n",
    "- **criterion**: Comme son nom l'indique c'est le critère de qualité que notre modèle utilise pour séparer les données et de créer les noeuds. Nous allons optimiser le modèle avec les valeurs suivantes: **[“gini”, “entropy”, “log_loss”]**\n",
    "- **max_depth**: la profondeur maximale que nous autorisons à notre arbre. Nous allons optimiser le modèle avec les valeurs suivantes: **[10,100,1000,None]**\n",
    "- **splitter**: C'est la stratégie que notre modèle utilise pour créer les noeuds. Nous allons optimiser le modèle avec les valeurs suivantes: **[“best”, “random”]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid={'criterion':[\"gini\", \"entropy\", \"log_loss\"],\n",
    "             'max_depth':[5,10],\n",
    "             'splitter': [\"best\", \"random\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [5, 10], &#x27;splitter&#x27;: [&#x27;best&#x27;, &#x27;random&#x27;]},\n",
       "             scoring=make_scorer(accuracy_score))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [5, 10], &#x27;splitter&#x27;: [&#x27;best&#x27;, &#x27;random&#x27;]},\n",
       "             scoring=make_scorer(accuracy_score))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42),\n",
       "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                         'max_depth': [5, 10], 'splitter': ['best', 'random']},\n",
       "             scoring=make_scorer(accuracy_score))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "\n",
    "# Initialize the Decision tree classifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# On creer un scorer pour le grid search \n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Initialisation de la GridSearch pour trouver le meilleur C.\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params_grid, scoring=scorer, cv=5)\n",
    "\n",
    "# On lance les multiples entrainements.\n",
    "grid_search.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get the best combination of parameters\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m      3\u001b[0m best_score \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_score_\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the best combination of parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Afin d'évaluer le modèle nous faisons une représentation graphique des résultats obtenus. Nous allons étudier les metrics suivants:\n",
    "- le **temps d'entrainement** que le modèle a besoin, ainsi que son **taux de mémoire (Mo)**.\n",
    "- **matrice de confusion**, qui est un excellent choix d'évaluation de performance, car nous sommes dans le cas d'une classification binaire. \n",
    "- **L'accuracy** du model.\n",
    "- **La précision**\n",
    "- **recall**\n",
    "- **F1-score**.\n",
    "- **ROC-AUC score**\n",
    "\n",
    "src: https://www.v7labs.com/blog/performance-metrics-in-machine-learning#h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation du modèle avec les meilleurs paramètres\n",
    "best_model=DecisionTreeClassifier(#C=best_params['criterion'],\n",
    "                                  max_depth=best_params['max_depth'],\n",
    "                                  splitter=best_params['splitter'],\n",
    "                                  random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voullons étudier **le temps d'entrainement (en h/m/s)** et **l'utilisation de la mémoire (Mo)** que le modèle a besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'entrainement (h | m | s) : 0 | 0 | 2\n",
      "Utilisation de la mémoire (Mo) : 0.00390625\n"
     ]
    }
   ],
   "source": [
    "def second_to_hms(seconds):\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return int(hours), int(minutes), int(seconds)\n",
    "\n",
    "def bytes_to_Mo(mem_bytes):\n",
    "    mem_kb = mem_bytes / 1024  # Convertir en kilooctets\n",
    "    mem_mb = mem_kb / 1024  # Convertir en mégaoctets\n",
    "    return mem_mb\n",
    "\n",
    "#lancemenet de l'enrestristrement de la mémoire.\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before_bytes = process.memory_info().rss\n",
    "\n",
    "#lancement de l'enregistrement du temps d'entrainement.\n",
    "start_time = time.time() \n",
    "# entrainement du modèle.\n",
    "best_model.fit(tfidf_train,y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "#le temps écoulé \n",
    "elapsed_time= end_time - start_time\n",
    "\n",
    "print(f\"Temps d'entrainement (h | m | s) : {second_to_hms(elapsed_time)[0]} | {second_to_hms(elapsed_time)[1]} | {second_to_hms(elapsed_time)[2]}\")\n",
    "\n",
    "mem_after_bytes = process.memory_info().rss\n",
    "#on convertit les bytes en Mo.\n",
    "mem_bytes=mem_after_bytes - mem_before_bytes\n",
    "\n",
    "\n",
    "print(f\"Utilisation de la mémoire (Mo) : {bytes_to_Mo(mem_bytes)}\")\n",
    "#Create or open a file with write-binary mode and save the model to it\n",
    "pickle = pkl.dump(model, open('DecisionTree_model.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#optention des prédictions de ce modèle.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecisionTree_model.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m y_pred\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(tfidf_test)\n\u001b[0;32m      4\u001b[0m y_pred\u001b[38;5;241m=\u001b[39mbest_model\u001b[38;5;241m.\u001b[39mpredict(tfidf_test)\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:499\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    477\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03m    For a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;124;03m        The predicted classes, or the predict values.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 499\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    500\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[0;32m    501\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1462\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This DecisionTreeClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "#optention des prédictions de ce modèle.\n",
    "best_model = pkl.load(open('DecisionTree_model.pickle', 'rb'))\n",
    "y_pred=model.predict(tfidf_test)\n",
    "y_pred=best_model.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation de la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#représentation graphique du résultat du meilleure model\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Les labels prédits')\n",
    "plt.ylabel('Les vrais labels')\n",
    "plt.title('Matrice de confusion')\n",
    "plt.show()\n",
    "\n",
    "print(f\"le modèle a donc {cm[0][0]} instances vrai Positives et {cm[1][1]} instances de Vrai négatives, {cm[1][0]} instances de Faux Positives et {cm[0][1]} instances de Faux négatives. \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=accuracy_score(y_pred,y_test)\n",
    "print(f\"Notre modèle a une précision globale de {round(acc*100,2)}%, ce qui signifie qu'il prédit correctement les classes des instances dans {round(acc*100,2)}% des cas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision=precision_score(y_test,y_pred)\n",
    "print(f\"notre model a une precision de {round(precision*100,2)}% . lorsqu'il prédit une classe comme positive, il a raison dans {round(precision*100,2)}% des cas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall=recall_score(y_test,y_pred)\n",
    "print(f\"notre model arrive a détecter {round(recall*100,2)}%. Donc sur l'ensemble des vrai positives, le modèle parvient à en identifier correctement {round(recall*100,2)}% des cas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score=f1_score(y_test,y_pred)\n",
    "print(f\"notre model a un F1-score de {round(f1score*100,2)}%, ce qui signifie que il y a un excellent equilibre entre le recall et la précision.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc=roc_auc_score(y_test,y_pred)\n",
    "print(f\"Le modèle a donc une score ROC-AUC de {roc_auc} ce qui est très proche de 1, donc le modèle a une excellente capacité à différencier entres les Fake et les vrais news.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('Taux de Faux Positives (%)')\n",
    "plt.ylabel('Taux de Vrai Positives (%)')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du vrAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrAI = 0\n",
    "acc = accuracy_score(y_pred,y_test)\n",
    "if acc > 0.90:\n",
    "    vrAI = 'Fx'\n",
    "elif  0.70<acc<=0.90:\n",
    "    vrAI = 'F'\n",
    "elif  0.50<acc<=0.70:\n",
    "    vrAI = 'E'\n",
    "elif  0.30<acc<=0.50:\n",
    "    vrAI = 'D'\n",
    "elif  0.15<acc<=0.30:\n",
    "    vrAI = 'C'\n",
    "elif  0.05<acc<=0.15:\n",
    "    vrAI = 'B'\n",
    "else:\n",
    "    vrAI = 'A'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
